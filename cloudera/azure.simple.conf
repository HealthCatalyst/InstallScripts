#
# Copyright (c) 2017 Cloudera, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

#
# Sample Cloudera Director configuration file based on the Cloudera Azure reference architecture:
# http://www.cloudera.com/documentation/other/reference-architecture/PDF/cloudera_ref_arch_azure.pdf
#
# Simple non-HA cluster with a single master node, 3 worker nodes
#


#
# Cluster name
# If environmentName and deploymentName are not defined they will get the value of 'name'.
# Must be unique.
#

name: C5-Simple-Azure

#
# Environment name
#

environmentName: Azure-nonHA

#
# Deployment name
# Used to name the Cloudera Manager instance in Cloudera Director.
# Must be unique.
#

deploymentName: Cloudera-Manager-on-Azure


#
# Cloud provider configuration (credentials, region, and management/authentication endpoints)
#

provider {
    type: azure

    #
    # ID of Azure region to use. NOTE: region must support Premium Storage
    # See: https://azure.microsoft.com/en-us/regions/#services
    #

    region: "region_REPLACE_ME"

    #
    # Azure Management URL.
    #

    mgmtUrl: "https://management.core.windows.net/"

    #
    # Azure Resource Manager URL.
    #

    armUrl: "https://management.azure.com/"

    #
    # Azure Active Directory Subscription ID.
    #

    subscriptionId: "subscriptionId_REPLACE_ME"

    #
    # Azure Active Directory URL.
    #

    aadUrl: "https://login.windows.net/"

    #
    # Tenant ID (from AAD)
    #

    tenantId: "tenantId_REPLACE_ME"

    #
    # Azure Active Directory Application Client ID.
    #

    clientId: "clientId_REPLACE_ME"

    #
    # Client Secret
    #

    clientSecret: "clientSecret_REPLACE_ME"
}


#
# SSH credentials to use to connect to the machines
#

ssh {
    username: "username_REPLACE_ME"
    privateKey: """-----BEGIN RSA PRIVATE KEY-----
privateKey_REPLACE_ME
-----END RSA PRIVATE KEY-----"""
}


#
# Common variable definitions
#
# These are the key / value pairs used to define instance templates. They are defined here and
# referenced throughout this config file.
# More about HOCON substitution: https://github.com/typesafehub/config/blob/master/HOCON.md#substitutions
#
# The format of this section:
#   - base: represents the core config fields that are common to all nodes
#   - master: represents the core config fields that are common to master nodes
#   - worker: represents the core config fields that are common to worker nodes
#   - edge: represents the core config fields that are common to edge nodes
#
#
# Instance Template Breakdown
# An instance template configuration consists of the following fields. Unless otherwise specified,
# all fields are required.
#
# - image: The image ID used for instances is an alias defined in the plugin configuration file.
#
# - type: The VM type. See the Azure RA for more detail.
#
# - computeResourceGroup: Resource Group for the deployment.  The Resource Group you specify must
#   exist within the region you selected.
#   See: https://azure.microsoft.com/en-us/documentation/articles/resource-group-overview/
#
# - networkSecurityGroupResourceGroup: The Resource Group for the Network Security Group. The
#   Resource Group you specify must exist within the region you selected.
#   See: https://azure.microsoft.com/en-us/documentation/articles/resource-group-overview/
#
# - networkSecurityGroup: The Network Security Group for this instance type, this has to be
#   within the networkSecurityGroupResourceGroup. NSG configuration allows you to limit access to
#   the VM with firewall-like rules.
#   See: https://azure.microsoft.com/en-us/documentation/articles/virtual-networks-nsg/
#
# - virtualNetworkResourceGroup: The Resource Group for the Virtual Network. The Resource Group you
#   specify must exist within the region you selected and should be the same for all instances that
#   will be used in the same cluster.
#   See: https://azure.microsoft.com/en-us/documentation/articles/resource-group-overview/
#
# - virtualNetwork: The Azure Virtual Network that will be used, this has to be within the
#   virtualNetworkResourceGroup and should be the same for all instances that will be used in the
#   same cluster.
#   See: https://azure.microsoft.com/en-us/documentation/services/virtual-network/
#
# - subnetName: The name of the Subnet that will be used, this has to be within the virtualNetwork.
#
# - instanceNamePrefix: Prefix for VM name and hostname of the VM. The VM name will have the
#   folloing format:
#       instanceNamePrefix-{UUID}
#   where {UUID} is generated by the Cloudera Director server.
#
# - hostFqdnSuffix: Hostname FQDN Suffix. This is the DNS domain you configured in your custom DNS
#   server. Example values are: cdh-cluster.internal, cluster.your-company-name.com. The host FQDN
#   is configured on the VMs with the following format:
#       {instanceNamePrefix}-{truncated-UUID}.hostFqdnSuffix
#
# - availabilitySet: Availability Set for this instance type.  Machines within the same availability
#   set will have staggared maintanance times. With a default availability set configuration no more
#   than 1/5 machines will be offline at a time (rounded up).
#   See: https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-linux-manage-availability/
#   Sharing Availability Set between master and worker nodes is strongly not recommended.
#
# - publicIP: Should this instance type have Azure Public IP Address and DNS Label?  If Yes, the
#   machine will have a publically resolvable hostname with the folling format:
#       {instanceNamePrefix}-{UUID}.{region}.coudapp.azure.com
#   Allowed values: Yes, No
#
# - storageAccountType: The storage account type to use. The dataDiskSize parameter should be
#   updated based on the storage account type used.
#   The current allowed values are:
#       - PremiumLRS
#       - StandardLRS
#   See the RA for the supported ways to use standard storage:
#   http://www.cloudera.com/documentation/other/reference-architecture/PDF/cloudera_ref_arch_azure.pdf
#
# - dataDiskCount: The number of data drives. The size can be specified with `dataDiskSize`
#   Data drives are mounted on /data0 .. /data[n]
#       /data0 - Dedicated Log Device
#   For Masters:
#       /data1 - NameNode Data
#       /data2 - Zookeeper Data / DataLog
#       /data3 - HDFS Secondary NameNode Data
#   For Workers:
#       /data1 .. /data[n] will be used for HDFS data
#
# - dataDiskSize: The size of each data drive.
#   For disks allocated in a premium storage account, only the following GB values are allowed:
#       512 (P20 disk)
#       1023 (P30 disk) (1023, not 1024)
#   For disks alllocated in a standard storage account, any size between 1 and 1023 inclusive can
#   be used.
#   See https://azure.microsoft.com/en-us/documentation/articles/storage-introduction/ and
#   https://azure.microsoft.com/en-us/documentation/articles/storage-premium-storage/
#
# - tags (optional): Additional tags to help label resources within Azure
#
# - bootstrapScript: The bootstrap script (see the bootstrap-script section)
#

common-instanceTemplate {

    # Core config fields that are common to all node types
    base {
        type: STANDARD_DS14
        image: cloudera-centos-6-latest
        networkSecurityGroupResourceGroup: "networkSecurityGroupResourceGroup_REPLACE_ME"
        networkSecurityGroup: "networkSecurityGroup_REPLACE_ME"
        virtualNetworkResourceGroup: "virtualNetworkResourceGroup_REPLACE_ME"
        virtualNetwork: "virtualNetwork_REPLACE_ME"
        subnetName: "subnetName_REPLACE_ME"
        hostFqdnSuffix: "hostFqdnSuffix_REPLACE_ME"
        tags {
            owner: ${?USER}
        }
    }

    # Core config fields that are common to all master nodes
    master {
        computeResourceGroup: "masterBase_computeResourceGroup_REPLACE_ME"
        availabilitySet: "master_availabilitySet_REPLACE_ME"
        instanceNamePrefix: "masterBase_instanceNamePrefix_REPLACE_ME"
        storageAccountType: "PremiumLRS"
        dataDiskCount: 4
        dataDiskSize: 512
        publicIP: No
    }

    # Config fields for worker nodes
    worker {
        computeResourceGroup: "worker_computeResourceGroup_REPLACE_ME"
        availabilitySet: "worker_availabilitySet_REPLACE_ME"
        instanceNamePrefix: "worker_instanceNamePrefix_REPLACE_ME"
        storageAccountType: "StandardLRS"
        dataDiskCount: 11
        dataDiskSize: 1023
        publicIP: No
    }

    # Config fields for edge nodes
    edge {
        computeResourceGroup: "edge_computeResourceGroup_REPLACE_ME"
        availabilitySet: "edge_availabilitySet_REPLACE_ME"
        instanceNamePrefix: "edge_instanceNamePrefix_REPLACE_ME"
        storageAccountType: "PremiumLRS"
        dataDiskCount: 1
        dataDiskSize: 512
        # Change this to Yes to allow accessing edge/CM nodes via public IP
        publicIP: No
    }
}


#
# Required external database server configuration.
#
# Cloudera Director can create databases on existing database servers.
# NOTE: Cloudera does not support Azure SQL DB service.
#
#
# Database Servers Breakdown
# A database server configuration consists of the following fields. All fields are required.
# To properly setup a database see http://www.cloudera.com/documentation/director/latest/topics/director_get_started_azure_set_up_msql_postgres.html
#
# - type: The type of database to use. Allowed values are:
#     - mysql
#     - postgresql
#
# - host: The static internal IP (recommended) or internal FQDN of the database server.
#
# - port: The database server's port. The default ports are:
#     - mysql: 3306
#     - postgresql: 5432
#
# - user: The user Cloudera Director will use. The user must have privileges to create databases,
#   create users, and grant the users access to those databases.
#
# - password: The password for the database user.
#

databaseServers {

    productionDB {
        type: "type_databaseServers_REPLACE_ME"
        host: "host_databaseServers_REPLACE_ME"
        port: "port_databaseServers_REPLACE_ME"
        user: "user_databaseServers_REPLACE_ME"
        password: "password_databaseServers_REPLACE_ME"
    }

    # Additional database servers can be added by following the same format.

} # End external database configs


#
# Bootstrap script
#
# The os-generic bootstrap script will be run after the VM boots up for the first time. This must
# be used to set up preconditions for successful cluster deployment. Director will restart the
# host after the bootstrap script has run.
#
# The example below is an os-generic script that supports these OSes:
#   - CentOS 6.7
#   - CentOS 7.2
#   - RHEL 6.7
#   - RHEL 7.2
#
# The script prepares the OS for cluster installation. It also configures a dhclient or
# NetworkManager hook (depending on OS) to register the A record and PTR record with the DNS server
# configured for the VNET to satisfy proper forward and reverse DNS resolution. Azure's default DNS
# currently does not support Reverse Lookup on private IP Addresses, which is a requirement for
# CDH. See the following link for an example BIND setup to satisfy this requirement:
# http://www.cloudera.com/documentation/director/latest/topics/director_get_started_azure_ddns.html
#
# The scrip also sets required settings for RHEL.
#

bootstrap-script {
    os-generic : """#!/bin/sh

#
# Copyright (c) 2017 Cloudera, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

#
# This script will bootstrap these OSes:
#   - CentOS 6.7
#   - CentOS 7.2
#   - RHEL 6.7
#   - RHEL 7.2
#
# Notes and notible differences between OSes:
#   - CentOS 6.7 and RHEL 6.7 use dhclient
#   - CentOS 7.2 and RHEL 7.2 use NetworkManager
#


#
# Functions
#

# writing dhclient-exit-hooks is the same for CentOS 6.x and RHEL 6.x
# function not indented so EOF works
dhclient_6()
{
# dhclient-exit-hooks explained in dhclient-script man page: http://linux.die.net/man/8/dhclient-script
# cat a here-doc represenation of the hooks to the appropriate file
cat > /etc/dhcp/dhclient-exit-hooks <<"EOF"
#!/bin/bash
printf "\ndhclient-exit-hooks running...\n\treason:%s\n\tinterface:%s\n" "${reason:?}" "${interface:?}"
# only execute on the primary nic
if [ "$interface" != "eth0" ]
then
    exit 0;
fi
# when we have a new IP, perform nsupdate
if [ "$reason" = BOUND ] || [ "$reason" = RENEW ] ||
[ "$reason" = REBIND ] || [ "$reason" = REBOOT ]
then
    printf "\tnew_ip_address:%s\n" "${new_ip_address:?}"
    host=$(hostname -s)
    domain=$(hostname | cut -d'.' -f2- -s)
    domain=${domain:='cdh-cluster.internal'} # REPLACE-ME If no hostname is provided, use cdh-cluster.internal
    IFS='.' read -ra ipparts <<< "$new_ip_address"
    ptrrec="$(printf %s "$new_ip_address." | tac -s.)in-addr.arpa"
    nsupdatecmds=$(mktemp -t nsupdate.XXXXXXXXXX)
    resolvconfupdate=$(mktemp -t resolvconfupdate.XXXXXXXXXX)
    echo updating resolv.conf
    grep -iv "search" /etc/resolv.conf > "$resolvconfupdate"
    echo "search $domain" >> "$resolvconfupdate"
    cat "$resolvconfupdate" > /etc/resolv.conf
    echo "Attempting to register $host.$domain and $ptrrec"
    {
        echo "update delete $host.$domain a"
        echo "update add $host.$domain 600 a $new_ip_address"
        echo "send"
        echo "update delete $ptrrec ptr"
        echo "update add $ptrrec 600 ptr $host.$domain"
        echo "send"
    } > "$nsupdatecmds"
    nsupdate "$nsupdatecmds"
fi
#done
exit 0;
EOF
chmod 755 /etc/dhcp/dhclient-exit-hooks
service network restart
}


centos_6x()
{
    echo "CentOS 6.x"

    # execute the CentOS 6.x / RHEL 6.x dhclient-exit-hooks setup
    dhclient_6
}


rhel_6x()
{
    echo "RHEL 6.x"

    # rewrite SELINUX config to disabled and turn off enforcement
    sed -i.bak "s/^SELINUX=.*$/SELINUX=disabled/" /etc/selinux/config
    setenforce 0
    # stop firewall and disable
    service iptables stop
    chkconfig iptables off
    # update config to disable IPv6 and disable
    echo "# Disable IPv6" >> /etc/sysctl.conf
    echo "net.ipv6.conf.all.disable_ipv6 = 1" >> /etc/sysctl.conf
    echo "net.ipv6.conf.default.disable_ipv6 = 1" >> /etc/sysctl.conf
    sysctl -w net.ipv6.conf.all.disable_ipv6=1
    sysctl -w net.ipv6.conf.default.disable_ipv6=1

    # execute the CentOS 6.x / RHEL 6.x dhclient-exit-hooks setup
    dhclient_6
}


# writing network manager hooks is the same for CentOS 7.2 and RHEL 7.2
# function not indented so EOF works
networkmanager_7()
{
# Centos 7.2 and RHEL 7.2 uses NetworkManager. Add a script to be automatically invoked when interface comes up.
cat > /etc/NetworkManager/dispatcher.d/12-register-dns <<"EOF"
#!/bin/bash
# NetworkManager Dispatch script
# Deployed by Cloudera Director Bootstrap
#
# Expected arguments:
#    $1 - interface
#    $2 - action
#
# See for info: http://linux.die.net/man/8/networkmanager

# Register A and PTR records when interface comes up
# only execute on the primary nic
if [ "$1" != "eth0" || "$2" != "up" ]
then
    exit 0;
fi

# when we have a new IP, perform nsupdate
new_ip_address="$DHCP4_IP_ADDRESS"

host=$(hostname -s)
domain=$(hostname | cut -d'.' -f2- -s)
domain=${domain:='cdh-cluster.internal'} # REPLACE-ME If no hostname is provided, use cdh-cluster.internal
IFS='.' read -ra ipparts <<< "$new_ip_address"
ptrrec="$(printf %s "$new_ip_address." | tac -s.)in-addr.arpa"
nsupdatecmds=$(mktemp -t nsupdate.XXXXXXXXXX)
resolvconfupdate=$(mktemp -t resolvconfupdate.XXXXXXXXXX)
echo updating resolv.conf
grep -iv "search" /etc/resolv.conf > "$resolvconfupdate"
echo "search $domain" >> "$resolvconfupdate"
cat "$resolvconfupdate" > /etc/resolv.conf
echo "Attempting to register $host.$domain and $ptrrec"
{
    echo "update delete $host.$domain a"
    echo "update add $host.$domain 600 a $new_ip_address"
    echo "send"
    echo "update delete $ptrrec ptr"
    echo "update add $ptrrec 600 ptr $host.$domain"
    echo "send"
} > "$nsupdatecmds"
nsupdate "$nsupdatecmds"
exit 0;
EOF
chmod 755 /etc/NetworkManager/dispatcher.d/12-register-dns
service network restart
}


centos_7x()
{
    echo "CentOS 7.x"

    # execute the CentOS 7.x / RHEL 7.x network manager setup
    networkmanager_7
}


rhel_7x()
{
    echo "RHEL 7.x"

    # rewrite SELINUX config to disable and turn off enforcement
    sed -i.bak "s/^SELINUX=.*$/SELINUX=disabled/" /etc/selinux/config
    setenforce 0
    # stop firewall and disable
    systemctl stop iptables
    systemctl iptables off
    # RHEL 7.x uses firewalld
    systemctl stop firewalld
    systemctl disable firewalld
    # Disable tuned so it does not overwrite sysctl.conf
    service tuned stop
    systemctl disable tuned
    # Disable chrony so it does not conflict with ntpd installed by Director
    systemctl stop chronyd
    systemctl disable chronyd
    # update config to disable IPv6 and disable
    echo "# Disable IPv6" >> /etc/sysctl.conf
    echo "net.ipv6.conf.all.disable_ipv6 = 1" >> /etc/sysctl.conf
    echo "net.ipv6.conf.default.disable_ipv6 = 1" >> /etc/sysctl.conf
    # swappniess is set by Director in /etc/sysctl.conf
    # Poke sysctl to have it pickup the config change.
    sysctl -p

    # execute the CentOS 7.x / RHEL 7.x network manager setup
    networkmanager_7
}


#
# Main workflow
#

# ensure user is root
if [ "$(id -u)" -ne 0 ]; then
    echo "Please run as root."
    exit 1
fi

# find the OS and release
os=""
release=""

# if it's there, use lsb_release
rpm -q redhat-lsb
if [ $? -eq 0 ]; then
    os=$(lsb_release -si)
    major_release=$(lsb_release -sr | cut -d '.' -f 1)

# if lsb_release isn't installed, use /etc/redhat-release
else
    grep  "CentOS.* 6\." /etc/redhat-release
    if [ $? -eq 0 ]; then
        os="CentOS"
        major_release="6"
    fi

    grep "CentOS.* 7\." /etc/redhat-release
    if [ $? -eq 0 ]; then
        os="CentOS"
        major_release="7"
    fi

    grep "Red Hat Enterprise Linux Server release 6\." /etc/redhat-release
    if [ $? -eq 0 ]; then
        os="RedHatEnterpriseServer"
        major_release="6"
    fi

    grep "Red Hat Enterprise Linux Server release 7\." /etc/redhat-release
    if [ $? -eq 0 ]; then
        os="RedHatEnterpriseServer"
        major_release="7"
    fi
fi

echo "OS: $os $major_release"

# select the OS and run the appropriate setup script
not_supported_msg="OS $os $release is not supported."
if [ "$os" = "CentOS" ]; then
    if [ "$major_release" = "6" ]; then
        centos_6x
    elif [ "$major_release" = "7" ]; then
        centos_7x
    else
        echo "$not_supported_msg"
        exit 1
    fi

elif [ "$os" = "RedHatEnterpriseServer" ]; then
    if [ "$major_release" = "6" ]; then
        rhel_6x
    elif [ "$major_release" = "7" ]; then
        rhel_7x
    else
        echo "$not_supported_msg"
        exit 1
    fi
else
    echo "$not_supported_msg"
    exit 1
fi
"""
} # end bootstrap-script


# Instant Templates

instances {

    master {
        image: ${?common-instanceTemplate.base.image}
        type: ${?common-instanceTemplate.base.type}
        computeResourceGroup: ${?common-instanceTemplate.master.computeResourceGroup}
        networkSecurityGroupResourceGroup: ${?common-instanceTemplate.base.networkSecurityGroupResourceGroup}
        networkSecurityGroup: ${?common-instanceTemplate.base.networkSecurityGroup}
        virtualNetworkResourceGroup: ${?common-instanceTemplate.base.virtualNetworkResourceGroup}
        virtualNetwork: ${?common-instanceTemplate.base.virtualNetwork}
        subnetName: ${?common-instanceTemplate.base.subnetName}
        instanceNamePrefix: ${?common-instanceTemplate.master.instanceNamePrefix}
        hostFqdnSuffix: ${?common-instanceTemplate.base.hostFqdnSuffix}
        availabilitySet: ${?common-instanceTemplate.master.availabilitySet}
        publicIP: ${?common-instanceTemplate.master.publicIP}
        storageAccountType: ${?common-instanceTemplate.master.storageAccountType}
        dataDiskCount: ${?common-instanceTemplate.master.dataDiskCount}
        dataDiskSize: ${?common-instanceTemplate.master.dataDiskSize}
        tags: ${?common-instanceTemplate.base.tags}
        bootstrapScripts: [ ${?bootstrap-script.os-generic} ]
    }

    worker {
        image: ${?common-instanceTemplate.base.image}
        type: ${?common-instanceTemplate.base.type}
        computeResourceGroup: ${?common-instanceTemplate.worker.computeResourceGroup}
        networkSecurityGroupResourceGroup: ${?common-instanceTemplate.base.networkSecurityGroupResourceGroup}
        networkSecurityGroup: ${?common-instanceTemplate.base.networkSecurityGroup}
        virtualNetworkResourceGroup: ${?common-instanceTemplate.base.virtualNetworkResourceGroup}
        virtualNetwork: ${?common-instanceTemplate.base.virtualNetwork}
        subnetName: ${?common-instanceTemplate.base.subnetName}
        instanceNamePrefix: ${?common-instanceTemplate.worker.instanceNamePrefix}
        hostFqdnSuffix: ${?common-instanceTemplate.base.hostFqdnSuffix}
        availabilitySet: ${?common-instanceTemplate.worker.availabilitySet}
        publicIP: ${?common-instanceTemplate.worker.publicIP}
        storageAccountType: ${?common-instanceTemplate.worker.storageAccountType}
        dataDiskCount: ${?common-instanceTemplate.worker.dataDiskCount}
        dataDiskSize: ${?common-instanceTemplate.worker.dataDiskSize}
        tags: ${?common-instanceTemplate.base.tags}
        bootstrapScripts: [ ${?bootstrap-script.os-generic} ]
    }

    edge {
        image: ${?common-instanceTemplate.base.image}
        type: ${?common-instanceTemplate.base.type}
        computeResourceGroup: ${?common-instanceTemplate.edge.computeResourceGroup}
        networkSecurityGroupResourceGroup: ${?common-instanceTemplate.base.networkSecurityGroupResourceGroup}
        networkSecurityGroup: ${?common-instanceTemplate.base.networkSecurityGroup}
        virtualNetworkResourceGroup: ${?common-instanceTemplate.base.virtualNetworkResourceGroup}
        virtualNetwork: ${?common-instanceTemplate.base.virtualNetwork}
        subnetName: ${?common-instanceTemplate.base.subnetName}
        instanceNamePrefix: ${?common-instanceTemplate.edge.instanceNamePrefix}
        hostFqdnSuffix: ${?common-instanceTemplate.base.hostFqdnSuffix}
        availabilitySet: ${?common-instanceTemplate.edge.availabilitySet}
        publicIP: ${?common-instanceTemplate.edge.publicIP}
        storageAccountType: ${?common-instanceTemplate.edge.storageAccountType}
        dataDiskCount: ${?common-instanceTemplate.edge.dataDiskCount}
        dataDiskSize: ${?common-instanceTemplate.edge.dataDiskSize}
        tags: ${?common-instanceTemplate.base.tags}
        bootstrapScripts: [ ${?bootstrap-script.os-generic} ]
    }

} # End instance templates


#
# Configuration for Cloudera Manager. Cloudera Director can use an existing Cloudera Manager
# or bootstrap everything from scratch for a new cluster
#

cloudera-manager {

    instance: ${instances.edge} {
        tags {
            application: "Cloudera Manager 5"
        }
    }

    #
    # Licensing configuration
    #
    # There are three mutually exclusive options for setting up Cloudera Manager's license.
    # 1. License text may be embedded in this file using the "license" field. Triple quotes (""")
    #    are recommended for including multi-line text strings.
    # 2. The "licensePath" can be used to specify the path to a file containing the license.
    # 3. The "enableEnterpriseTrial" flag indicates whether the 60-Day Cloudera Enterprise Trial
    #    should be activated when no license is present. This must not be set to true if a
    #    license is included using either "license" or "licensePath".

    #
    # Embed a license for Cloudera Manager
    #

    # license: """
    #   -----BEGIN PGP SIGNED MESSAGE-----
    #   Hash: SHA1
    #
    # {
    #   "version"        : 1,
    #   "name"           : "License Owner",
    #   "uuid"           : "license id",
    #   "expirationDate" : 0,
    #   "features"       : [ "FEATURE1", "FEATURE2" ]
    # }
    # -----BEGIN PGP SIGNATURE-----
    # Version: GnuPG v1.4.11 (GNU/Linux)
    #
    # PGP SIGNATURE
    # -----END PGP SIGNATURE-----
    # """

    #
    # Include a license for Cloudera Manager from an external file
    #

    # licensePath: "/path/to/license.txt.asc"

    #
    # Specify the billingId.
    #
    # Cloudera Director will use the billing ID to report usage information to a metering service
    # for usage based billing.
    #
    # Usage reporting starts as soon as you assign a billing ID and a license to a Cloudera Manager.
    # If you remove a billing ID, Director will stop reporting to the metering service.
    #
    # When usage reporting stops, you will not have access to Cloudera Support with this deployment.
    # If you want a billing ID, please contact Cloudera.
    #

    # billingId: "billingId_REPLACE_ME"

    #
    # Activate 60-Day Cloudera Enterprise Trial
    #

    enableEnterpriseTrial: true

    #
    # Install unlimited strength JCE policy files along with Cloudera Manager
    #

    # unlimitedJce: true

    #
    # Optional database configuration
    #
    # There are three mutually exclusive options for database usage in Cloudera Director.
    # 1. This option is NOT supported for production use.
    #    With no configuration, an embedded PostgreSQL database will be used.
    # 2. Alternatively, existing external databases can be used.
    # 3. Finally, databases can be created on the fly on existing external database servers.

    #
    # Optional configuration for existing external databases
    #
    # databases {
    #     CLOUDERA_MANAGER {
    #         type: postgresql
    #
    #         host: db.example.com
    #         port: 123
    #
    #         user: admin
    #         password: 1231ed
    #
    #         name: scm
    #     }
    #
    #     ACTIVITYMONITOR { ... }
    #
    #     REPORTSMANAGER { ... }
    #
    #     NAVIGATOR { ... }
    #
    #     # Added in Cloudera Manager 5.2+
    #     NAVIGATORMETASERVER { ... }
    # }

    #
    # Optional configuration for creating external databases on the fly
    #
    # When a database is created on the fly, Director generates a random database name using the specified database
    # name prefix, a random username based on the specified username prefix, and a random password. The password is
    # stored by Director and made available to the service that uses the database. If multiple services reference the
    # same external database server, Director will create a database for each.
    #
    # MySQL limits usernames to sixteen characters. Therefore, limit usernamePrefix values for databases on MySQL to
    # seven characters; the remaining nine characters are used by the randomized suffix generated by Director.
    #

    databaseTemplates {
        CLOUDERA_MANAGER {
            name: cmtemplate
            databaseServerName: productionDB # Must correspond to an external database server named above
            databaseNamePrefix: scm
            usernamePrefix: cmadmin
        }

        ACTIVITYMONITOR {
            name: amontemplate
            databaseServerName: productionDB # Must correspond to an external database server named above
            databaseNamePrefix: amon
            usernamePrefix: amadmin
        }

        REPORTSMANAGER {
            name: rmantemplate
            databaseServerName: productionDB # Must correspond to an external database server named above
            databaseNamePrefix: rman
            usernamePrefix: rmadmin
        }

        NAVIGATOR {
            name: navtemplate
            databaseServerName: productionDB # Must correspond to an external database server named above
            databaseNamePrefix: nav
            usernamePrefix: nadmin
        }

        # Added in Cloudera Manager 5.2+
        NAVIGATORMETASERVER {
            name: navmetatemplate
            databaseServerName: productionDB # Must correspond to an external database server named above
            databaseNamePrefix: navmeta
            usernamePrefix: nmadmin
        }
    }

    #
    # Configuration to override Cloudera Manager package repositories
    #

    # repository: "http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/5.12/"
    # repositoryKeyUrl: "http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/RPM-GPG-KEY-cloudera"

    # OR use an existing Cloudera Manager installation

    # hostname: "192.168.33.10"
    # username: <if not default 'admin'>
    # password: <if not default 'admin'>

    #
    # Optional configuration for Cloudera Manager and its management services
    #
    # Configuration properties for CLOUDERA_MANAGER are documented at
    # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_cmserver.html
    #
    # Configuration properties for the Cloudera Management services are documented at
    # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_mgmtservice.html
    #
    # Configuration properties for Hosts are documented at
    # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_host.html
    #

    configs {
        # CLOUDERA_MANAGER corresponds to the Cloudera Manager Server configuration options
        CLOUDERA_MANAGER {
            # enable_api_debug: false
            custom_banner_html: "Managed by Cloudera Director"
        }

        # CLOUDERA_MANAGEMENT_SERVICE corresponds to the Service-Wide configuration options
        CLOUDERA_MANAGEMENT_SERVICE {
            # enable_alerts : false
            # enable_config_alerts : false
        }

        SERVICEMONITOR {
            mgmt_log_dir:/data0/log/cloudera-scm-firehose
            firehose_storage_dir:/data0/lib/cloudera-service-monitor
        }

        ACTIVITYMONITOR {
            mgmt_log_dir:/data0/log/cloudera-scm-firehose
        }

        HOSTMONITOR {
            mgmt_log_dir: /data0/log/cloudera-scm-firehose
            firehose_storage_dir: /data0/lib/cloudera-host-monitor
        }

        REPORTSMANAGER {
            headlamp_scratch_dir: /data0/lib/cloudera-scm-headlamp
            mgmt_log_dir: /data0/log/cloudera-scm-headlamp
        }

        EVENTSERVER {
            mgmt_log_dir:/data0/log/cloudera-scm-eventserver
            eventserver_index_dir:/data0/lib/cloudera-scm-eventserver
        }

        ALERTPUBLISHER {
            mgmt_log_dir:/data0/log/cloudera-scm-alertpublisher
        }

        NAVIGATOR {
            mgmt_log_dir:/data0/log/cloudera-scm-navigator
        }

        NAVIGATORMETASERVER {
            audit_event_log_dir:/data0/log/cloudera-scm-navigator/audit
            data_dir:/data0/lib/cloudera-scm-navigator
            mgmt_log_dir:/data0/log/cloudera-scm-navigator
        }

        # Configuration properties for all hosts
        HOSTS {
        }
    }
} # End CM configuration


#
# Simple single-master Cluster description
#

cluster {

    # List the products and their versions that need to be installed.
    # These products must have a corresponding parcel in the parcelRepositories
    # configured above. The specified version will be used to find a suitable
    # parcel. Specifying a version that points to more than one parcel among
    # those available will result in a configuration error. Specify more granular
    # versions to avoid conflicts.

    products {
        CDH: 5
    }

    #
    # Optional override of CDH parcel repositories
    #

    # parcelRepositories: ["http://archive.cloudera.com/cdh5/parcels/5.12/"]

    services: [HDFS, YARN, ZOOKEEPER, HBASE, HIVE, HUE, IMPALA, OOZIE, SPARK_ON_YARN]

    #
    # Optional custom service configurations
    # Configuration keys containing periods must be enclosed in double quotes.
    #

    configs {
        # HDFS fencing should be set to true for HA configurations
        HDFS {
            dfs_replication: "3"
            dfs_block_local_path_access_user: "impala,hbase,mapred,spark"
        }

        HIVE {
            audit_event_log_dir: /data0/log/hive/audit
            lineage_event_log_dir: /data0/log/hive/lineage
        }

        HBASE {
            audit_event_log_dir: /data0/log/hbase/audit
        }
    }

    #
    # Optional configuration for existing external database for Hive Metastore, Hue,
    # and Oozie databases
    #

    # databases {
    #     HIVE {
    #         type: postgresql
    #         host: db.example.com
    #         port: 123
    #         user: hive
    #         password: pass
    #         name: hive_db
    #     }
    #     HUE {
    #         type: postgresql
    #         host: db.example.com
    #         port: 123
    #         user: hue
    #         password: pass
    #         name: hue_db
    #     }
    #     OOZIE {
    #         type: postgresql
    #         host: db.example.com
    #         port: 123
    #         user: oozie
    #         password: pass
    #         name: oozie_db
    #     }
    # }

    #
    # Optional configuration for creating external database on the fly for Hive Metastore
    # Hue, and Oozie databases
    #

    databaseTemplates: {
        HIVE {
            name: hivetemplate
            databaseServerName: productionDB # Must correspond to an external database server named above
            databaseNamePrefix: hivemetastore
            usernamePrefix: hive
        }

        HUE {
            name: huetemplate
            databaseServerName: productionDB # Must correspond to an external database server named above
            databaseNamePrefix: huedb
            usernamePrefix: hue
        }

        OOZIE {
            name: oozietemplate
            databaseServerName: productionDB # Must correspond to an external database server named above
            databaseNamePrefix: ooziedb
            usernamePrefix: oozie
        }
    }

    #
    # This reference configuration follows the Cloudera Azure Reference Architecture, modified
    # for a simple single-master cluster.
    #

    masters {
        count: 1

        instance: ${instances.master} {
            tags {
                group: masters
            }
        }

        roles {
            ZOOKEEPER: [SERVER]
            HDFS: [NAMENODE, SECONDARYNAMENODE]
            YARN: [RESOURCEMANAGER, JOBHISTORY]
            HBASE: [MASTER, HBASETHRIFTSERVER] # Alternately [HBASERESTSERVER], for HUE Integration
            HUE: [HUE_SERVER]
            OOZIE: [OOZIE_SERVER]
            IMPALA: [CATALOGSERVER, STATESTORE]
            HIVE: [HIVESERVER2, HIVEMETASTORE, WEBHCAT]
            SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER]
        }

        configs {
            HDFS {
                NAMENODE {
                    namenode_log_dir: /data0/log/hadoop-hdfs
                    dfs_name_dir_list: /data1/dfs/nn
                }
                SECONDARYNAMENODE {
                    secondarynamenode_log_dir: /data0/log/hadoop-hdfs
                    fs_checkpoint_dir_list: /data3/dfs/snn
                }
            }
            ZOOKEEPER {
                SERVER {
                    zk_server_log_dir: /data0/log/zookeeper
                    dataDir: /data2/zookeeper
                    dataLogDir: /data2/zookeeper
                    maxClientCnxns: 1024
                }
            }
            YARN {
              RESOURCEMANAGER {
                resource_manager_log_dir: /data0/log/hadoop-yarn
              }
              JOBHISTORY {
                  mr2_jobhistory_log_dir: /data0/log/hadoop-mapreduce
              }
            }
            HBASE {
              MASTER {
                  hbase_master_log_dir: /data0/log/hbase
              }
              HBASETHRIFTSERVER {
                  hbase_thriftserver_log_dir: /data0/log/hbase
              }
              #HBASERESTSERVER {
              #    hbase_restserver_log_dir: /data0/log/hbase
              #}
            }
            HIVE {
                HIVEMETASTORE {
                    hive_log_dir: /data0/log/hive
                }
                HIVESERVER2 {
                    hive_log_dir: /data0/log/hive
                }
                WEBHCAT {
                    hcatalog_log_dir: /data0/log/hcatalog
                }
            }
            OOZIE {
                OOZIE_SERVER {
                    oozie_plugins_list: "org.apache.oozie.service.ZKLocksService,org.apache.oozie.service.ZKXLogStreamingService,org.apache.oozie.service.ZKJobsConcurrencyService,org.apache.oozie.service.ZKUUIDService"
                    oozie_log_dir: /data0/log/oozie
                }
            }
            HUE {
                HUE_SERVER {
                    hue_server_log_dir: /data0/log/hue
                }
            }
            IMPALA {
                CATALOGSERVER {
                    log_dir: /data0/log/catalogd
                }
                STATESTORE {
                    log_dir: /data0/log/statestore
                }
            }
            SPARK_ON_YARN {
                SPARK_YARN_HISTORY_SERVER {
                    log_dir: /data0/log/spark
                }
            }
            #HBASE {
            #}
        }
    }

    workers {
        #
        # The desired number of instances to provision. Cloudera Director will attempt to allocate
        # this many instances but will not fail the deployment as long as the minimum number of
        # instances (specified with minCount below) is allocated. If minCount is not specified
        # then minCount is set to count.
        #
        count: 3

        #
        # Minimum number of instances required to set up the cluster.
        # Fail and quit if minCount number of instances is not available in this cloud
        # environment. Else, continue setting up the cluster. If minCount is not defined then
        # minCount defaults to count.
        #
        minCount: 3

        instance: ${instances.worker} {
            tags {
                group: worker
            }
        }

        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            HBASE: [REGIONSERVER]
            IMPALA: [IMPALAD]
        }

        # Optional custom role configurations
        # Configuration keys containing periods must be enclosed in double quotes.
        configs {
            HDFS {
                DATANODE {
                    datanode_log_dir: /data0/log/hadoop-hdfs
                    dfs_data_dir_list: "/data1/dfs/dn,/data2/dfs/dn,/data3/dfs/dn,/data4/dfs/dn,/data5/dfs/dn,/data6/dfs/dn,/data7/dfs/dn,/data8/dfs/dn,/data9/dfs/dn,/data10/dfs/dn"
                    dfs_datanode_failed_volumes_tolerated: 1
                }
            }
            YARN {
                NODEMANAGER {
                    node_manager_log_dir: /data0/log/hadoop-yarn
                    yarn_nodemanager_log_dirs: "/data1/log/hadoop-yarn/container,/data2/log/hadoop-yarn/container,/data3/log/hadoop-yarn/container,/data4/log/hadoop-yarn/container,/data5/log/hadoop-yarn/container,/data6/log/hadoop-yarn/container,/data7/log/hadoop-yarn/container,/data8/log/hadoop-yarn/container,/data9/log/hadoop-yarn/container,/data10/log/hadoop-yarn/container"
                    yarn_nodemanager_local_dirs: "/data1/yarn,/data2/yarn,/data3/yarn,/data4/yarn,/data5/yarn,/data6/yarn,/data7/yarn,/data8/yarn,/data9/yarn,/data10/yarn"
                }
            }
            HBASE {
                REGIONSERVER {
                    hbase_regionserver_log_dir: /data0/log/hbase
                }
            }
            IMPALA {
                IMPALAD {
                    log_dir: /data0/log/impalad
                    lineage_event_log_dir: /data0/log/impalad/lineage
                    audit_event_log_dir: /data0/log/impalad/audit
                    scratch_dirs: "/data1/impala/impalad,/data2/impala/impalad,/data3/impala/impalad,/data4/impala/impalad,/data5/impala/impalad,/data6/impala/impalad,/data7/impala/impalad,/data8/impala/impalad,/data9/impala/impalad,/data10/impala/impalad"
                }
            }
        }
    }

    postCreateScripts: ["""#!/bin/sh

# This is an embedded post creation script that runs as root and can be used to
# customize the cluster after it has been created.

# If the exit code is not zero Cloudera Director will fail

# Post creation scripts also have access to the following environment variables:

#    DEPLOYMENT_HOST_PORT
#    ENVIRONMENT_NAME
#    DEPLOYMENT_NAME
#    CLUSTER_NAME
#    CM_USERNAME
#    CM_PASSWORD

echo 'Hello World!'
exit 0
    """,
    """#!/usr/bin/python

# Additionally, multiple post-creation scripts can be supplied.  They will run
# in the order they are listed here.  Interpeters other than bash can be used
# as well.

print 'Hello again!'
    """]

    # For more complex scripts, post creation scripts can be supplied via path,
    # where they will be read from the local filesystem.  They will run after
    # any scripts supplied in the previous postCreateScripts section.
    # postCreateScriptsPaths: ["/tmp/test-script.sh",
    #                         "/tmp/test-script.py"]

    preTerminateScripts: ["""#!/bin/sh

# This is an embedded pre-termination script that runs as root and can be used to
# customize the cluster after it has been created.

# If the exit code is not zero Cloudera Director will fail

# Pre terminate scripts also have access to the following environment variables:

#    DEPLOYMENT_HOST_PORT
#    ENVIRONMENT_NAME
#    DEPLOYMENT_NAME
#    CLUSTER_NAME
#    CM_USERNAME
#    CM_PASSWORD

echo 'Goodbye World!'
exit 0
    """,
    """#!/usr/bin/python

# Additionally, multiple pre terminate scripts can be supplied.  They will run
# in the order they are listed here.  Interpeters other than bash can be used
# as well.

print 'Goodbye again!'
        """]

    # For more complex scripts, pre terminate scripts can be supplied via path,
    # where they will be read from the local filesystem.  They will run after
    # any scripts supplied in the previous preTerminateScripts section.
    # preTerminateScriptsPaths: ["/tmp/test-script.sh",
    #                            "/tmp/test-script.py"]
}
